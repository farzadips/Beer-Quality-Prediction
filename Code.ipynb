{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "T83Qs1LLL50A"
   },
   "source": [
    "Farzad Imanpour Sardroudi\n",
    "Politecnico di Torino\n",
    "Student id: s289265\n",
    "s289265@studenti.polito.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Qd6_WZ4FHs5"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKf4fWlpCwbK",
    "outputId": "3a50e2bb-bc86-4535-9d47-9c3fbddb776d"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression,SGDRegressor, Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import scipy\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn import preprocessing as preprocessingsk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import text\n",
    "from Orange.data import Table\n",
    "from Orange.preprocess import Impute, Model\n",
    "from Orange.modelling import TreeLearner\n",
    "from Orange.data.pandas_compat import table_from_frame,table_to_frame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#import category_encoders as cat\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwods')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install orange3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDakdoqnFLdH"
   },
   "source": [
    "## Methods and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5VeVVJECyjz"
   },
   "outputs": [],
   "source": [
    "##################################  \n",
    "## General methods\n",
    "##################################\n",
    "\n",
    "def loadData(directory):\n",
    "    df = pd.read_csv(directory,sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def get_final_csv(ids, y, filename):\n",
    "    pd.DataFrame(dict(Id = ids,Predicted = y)).to_csv(filename,sep=\",\",index=False)\n",
    "\n",
    "    \n",
    "models = [LinearRegression(),Ridge(), SGDRegressor()]\n",
    "targets = [\"LinearRegression\",\"Ridge\",\"SGDRegressor\"]\n",
    "def evaluateModels(models, targets,X,y):\n",
    "    scores = pd.DataFrame()\n",
    "    for model,target in zip(models,targets):\n",
    "        scores[target] = cross_val_score(model, X, y, scoring='r2', cv=3, n_jobs=-1)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "##################################  \n",
    "## Encoding and Missing values\n",
    "##################################\n",
    "def impute_gender(X_dev,X_eval):\n",
    "    X_dev['mean_of_four_gender'] = (X_dev['aroma'] + X_dev['appearance'] + X_dev['palate'] + X_dev['taste'] )/4\n",
    "    x_m = X_dev[X_dev['gender'] == \"Male\"]\n",
    "    x_f = X_dev[X_dev['gender'] == \"Female\"]\n",
    "    meanm = (x_m['aroma'].mean() + x_m['appearance'].mean() + x_m['palate'].mean() + x_m['taste'].mean() )/4\n",
    "    meanf = (x_f['aroma'].mean() + x_f['appearance'].mean() + x_f['palate'].mean() + x_f['taste'].mean() )/4\n",
    "    X_dev['gender'].loc[(abs(X_dev['mean_of_four_gender'] - meanm) \n",
    "                         <= abs(X_dev['mean_of_four_gender'] - meanf)  ) & (X_dev['gender'].isnull())] = 10  \n",
    "    X_dev['gender'].loc[(abs(X_dev['mean_of_four_gender'] - meanm) \n",
    "                         > abs(X_dev['mean_of_four_gender'] - meanf)  ) & (X_dev['gender'].isnull())] = 20  \n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  10)] = \"Male\"\n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  20)] = \"Female\"\n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  'Male')] = 0\n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  'Female')] = 1\n",
    "    ################################################\n",
    "    ################################################\n",
    "    X_eval['mean_of_four_gender'] = (X_eval['aroma'] + X_eval['appearance'] + X_eval['palate'] + X_eval['taste'] )/4\n",
    "    x_m = X_eval[X_eval['gender'] == \"Male\"]\n",
    "    x_f = X_eval[X_eval['gender'] == \"Female\"]\n",
    "    meanm = (x_m['aroma'].mean() + x_m['appearance'].mean() + x_m['palate'].mean() + x_m['taste'].mean() )/4\n",
    "    meanf = (x_f['aroma'].mean() + x_f['appearance'].mean() + x_f['palate'].mean() + x_f['taste'].mean() )/4\n",
    "    X_eval['gender'].loc[(abs(X_eval['mean_of_four_gender'] - meanm) \n",
    "                         <= abs(X_eval['mean_of_four_gender'] - meanf)  ) & (X_eval['gender'].isnull())] = 10  \n",
    "    X_eval['gender'].loc[(abs(X_eval['mean_of_four_gender'] - meanm) \n",
    "                         > abs(X_eval['mean_of_four_gender'] - meanf)  ) & (X_eval['gender'].isnull())] = 20  \n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  10)] = \"Male\"\n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  20)] = \"Female\"\n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  'Male')] = 0\n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  'Female')] = 1\n",
    "    return X_dev['gender'],X_eval['gender']\n",
    "\n",
    "    ##########################\n",
    "    ##########################\n",
    "def impute_orange(X_dev,X_eval):\n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  'Male')] = 0\n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  'Female')] = 0\n",
    "    \n",
    "    data = Table(X_dev.drop(columns=[\"name\",\"style\",\"birthdayRaw\",\"profileName\"]))\n",
    "    imputer = Impute(method=Model(TreeLearner()))\n",
    "    impute_heart = imputer(data)\n",
    "    df= table_to_frame(impute_heart)\n",
    "    X_dev[\"ABV\"] = df[\"Feature 1\"]\n",
    "    X_dev[\"appearance\"] = df[\"Feature 2\"]\n",
    "    X_dev[\"aroma\"] = df[\"Feature 3\"]\n",
    "    X_dev[\"overall\"] = df[\"Feature 4\"]\n",
    "    X_dev[\"palate\"] = df[\"Feature 5\"]\n",
    "    X_dev[\"taste\"] = df[\"Feature 6\"]\n",
    "    X_dev[\"ageInSeconds\"] = df[\"Feature 7\"]\n",
    "    X_dev[\"birthdayUnix\"] = df[\"Feature 8\"]\n",
    "    X_dev[\"gender\"] = df[\"Feature 9\"]\n",
    "    ##################\n",
    "    ##################\n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  'Male')] = 0\n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  'Female')] = 0\n",
    "    \n",
    "    data = Table(X_eval.drop(columns=[\"name\",\"style\",\"birthdayRaw\",\"profileName\"]))\n",
    "    imputer = Impute(method=Model(TreeLearner()))\n",
    "    impute_heart = imputer(data)\n",
    "    df= table_to_frame(impute_heart)\n",
    "    X_eval[\"ABV\"] = df[\"Feature 1\"]\n",
    "    X_eval[\"appearance\"] = df[\"Feature 2\"]\n",
    "    X_eval[\"aroma\"] = df[\"Feature 3\"]\n",
    "    X_eval[\"palate\"] = df[\"Feature 4\"]\n",
    "    X_eval[\"taste\"] = df[\"Feature 5\"]\n",
    "    X_eval[\"ageInSeconds\"] = df[\"Feature 6\"]\n",
    "    X_eval[\"birthdayUnix\"] = df[\"Feature 7\"]\n",
    "    X_eval[\"gender\"] = df[\"Feature 8\"]\n",
    "    return X_dev,X_eval\n",
    "    ##########################\n",
    "    ##########################\n",
    "def MonthTonum(str):\n",
    "    if(str == 'Jan'):return 1\n",
    "    if(str == 'Feb'):return 2\n",
    "    if(str == 'Mar'):return 3\n",
    "    if(str == 'Apr'):return 4\n",
    "    if(str == 'May'):return 5\n",
    "    if(str == 'Jun'):return 6\n",
    "    if(str == 'Jul'):return 7\n",
    "    if(str == 'aug'):return 8\n",
    "    if(str == 'Sep'):return 9\n",
    "    if(str == 'Oct'):return 10\n",
    "    if(str == 'Nov'):return 11\n",
    "    if(str == 'Dec'):return 12\n",
    "    else: return 0\n",
    "###########################\n",
    "###########################\n",
    "###########################\n",
    "def KNNimp(X_dev,X_eval):\n",
    "    print(\"X_dev KNNIMPUR STARTED\")\n",
    "    X_dev['birthdayRaw'].loc[(X_dev['birthdayRaw'].isna()==True)] = '0'\n",
    "    X_dev['birthdayRaw'] = X_dev['birthdayRaw'].apply(lambda int_date: (int(((str(int_date)).split(',')[0])[-2:].strip() + (str(int_date))[-4:]+str((MonthTonum(((str(int_date)).split(',')[0].split(' ')[0].strip())))))))\n",
    "    X_dev['birthdayRaw'].loc[(X_dev['birthdayRaw']==0)] = np.nan\n",
    "    \n",
    "    \n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  'Male')] = 0\n",
    "    X_dev['gender'].loc[(X_dev['gender'] ==  'Female')] = 1\n",
    "    \n",
    "    df_dev = X_dev\n",
    "    df_dev = df_dev.drop(columns = ['text','name','profileName','style'])\n",
    "    \n",
    "   \n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df1 = df_dev\n",
    "    df1 = df1.drop(columns = ['overall'])\n",
    "    df1 = pd.DataFrame(scaler.fit_transform(df1), columns = df1.columns)\n",
    "    df_dev = pd.concat([df_dev['overall'], df1], axis=1)\n",
    "    \n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=500000)\n",
    "    df_dev = pd.DataFrame(imputer.fit_transform(df_dev),columns = df_dev.columns)\n",
    "    print(\"X_dev KNNIMPUR ENDED\")\n",
    "    ###################\n",
    "    print(\"X_eval KNNIMPUR STARTED\")\n",
    "\n",
    "    X_eval['birthdayRaw'].loc[(X_eval['birthdayRaw'].isna()==True)] = '0'\n",
    "    X_eval['birthdayRaw'] = X_eval['birthdayRaw'].apply(lambda int_date: (int(((str(int_date)).split(',')[0])[-2:].strip() + (str(int_date))[-4:]+str((MonthTonum(((str(int_date)).split(',')[0].split(' ')[0].strip())))))))\n",
    "    X_eval['birthdayRaw'].loc[(X_eval['birthdayRaw']==0)] = np.nan\n",
    "    \n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  'Male')] = 0\n",
    "    X_eval['gender'].loc[(X_eval['gender'] ==  'Female')] = 1\n",
    "    \n",
    "    df_eval = X_eval\n",
    "    df_eval = df_eval.drop(columns = ['text','name','profileName','style'])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df_eval = pd.DataFrame(scaler.fit_transform(df_eval), columns = df_eval.columns)\n",
    "    df_eval.head()\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=50)\n",
    "    df_eval = pd.DataFrame(imputer.fit_transform(df_eval),columns = df_eval.columns)\n",
    "    print(\"X_eval KNNIMPUR ENDED\")\n",
    "    return df_dev,df_eval\n",
    "    \n",
    "def preprocessing(X_d,X_e):\n",
    "    # region_2 has too many duplicates\n",
    "    # description will be manipulated separately\n",
    "    imputer2 = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    #X_d[\"overall\"] = imputer2.fit_transform(np.array(X_d[\"overall\"]).reshape(-1,1))\n",
    "    \n",
    "    X_d1,X_e1 = KNNimp(X_d,X_e)\n",
    "\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    imputer2 = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    #I didn't use Simple imputer or Orange and isntead I used KNN\n",
    "    #X_d[\"birthdayRaw\"] = imputer.fit_transform(np.array(X_d[\"birthdayRaw\"]).reshape(-1,1))\n",
    "    #X_d[\"gender\"] = imputer.fit_transform(np.array(X_d[\"gender\"]).reshape(-1,1))\n",
    "    #X_d[\"profileName\"] = imputer.fit_transform(np.array(X_d[\"profileName\"]).reshape(-1,1))\n",
    "    #X_d[\"ABV\"] = imputer.fit_transform(np.array(X_d[\"ABV\"]).reshape(-1,1))\n",
    "    #X_d[\"ageInSeconds\"]  = imputer2.fit_transform(np.array(X_d[\"ageInSeconds\"] / 60 / 60 / 24 / 365.25).reshape(-1,1))\n",
    "    #X_d[\"ageInSeconds\"] = round(X_d[\"ageInSeconds\"], 5)\n",
    "    #X_d[\"birthdayUnix\"]  = imputer2.fit_transform((np.array(X_d[\"birthdayUnix\"])).reshape(-1,1))\n",
    "    #X_d[\"birthdayUnix\"] = preprocessingsk.normalize((np.array(X_d[\"birthdayUnix\"])).reshape(-1,1),'max',0)\n",
    "    #X_d[\"birthdayUnix\"] = round(X_d[\"birthdayUnix\"], 5)\n",
    "    #################################\n",
    "    #X_e[\"birthdayRaw\"] = imputer.fit_transform(np.array(X_e[\"birthdayRaw\"]).reshape(-1,1))\n",
    "    #X_e[\"gender\"] = imputer.fit_transform(np.array(X_e[\"gender\"]).reshape(-1,1))\n",
    "    #X_e[\"profileName\"] = imputer.fit_transform(np.array(X_e[\"profileName\"]).reshape(-1,1))\n",
    "    #X_e[\"ABV\"] = imputer.fit_transform(np.array(X_e[\"ABV\"]).reshape(-1,1))\n",
    "    #X_e[\"ageInSeconds\"]  = imputer2.fit_transform(np.array(X_e[\"ageInSeconds\"] / 60 / 60 / 24 / 365.25).reshape(-1,1))\n",
    "    #X_e[\"ageInSeconds\"] = round(X_e[\"ageInSeconds\"], 5)\n",
    "    #X_e[\"birthdayUnix\"]  = imputer2.fit_transform((np.array(X_e[\"birthdayUnix\"])).reshape(-1,1))\n",
    "    #X_e[\"birthdayUnix\"] = preprocessingsk.normalize((np.array(X_e[\"birthdayUnix\"])).reshape(-1,1),'max',0)\n",
    "    #X_e[\"birthdayUnix\"] = round(X_e[\"birthdayUnix\"], 5)\n",
    "\n",
    "\n",
    "\n",
    "    X_d = pd.concat([X_d1, X_d[['name','profileName','style']]], axis=1)\n",
    "    X_e = pd.concat([X_e1, X_e[['name','profileName','style']]], axis=1)\n",
    "    y = X_d.overall\n",
    "    X_d = X_d.drop(columns=[\"overall\"])\n",
    "   \n",
    "    # concat dev and eval for the encoding \n",
    "\n",
    "\n",
    "    df = pd.concat([X_d,X_e])\n",
    "    \n",
    "    # encode and use the sparse matrix because pandas' df is too heavy\n",
    "    df_enc = pd.get_dummies(df)\n",
    "    \n",
    "    df_enc_scipy = scipy.sparse.csr_matrix(df_enc.values)\n",
    "    \n",
    "    # split and return the encoded values\n",
    "    return df_enc_scipy[:X_d.shape[0]], y, df_enc_scipy[X_d.shape[0]:],df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI_DM_XMCzq6"
   },
   "outputs": [],
   "source": [
    "##################################  \n",
    "## Document preprocessing\n",
    "##################################\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "# general structure learnt from Lab10\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        \n",
    "        for t in word_tokenize(document):\n",
    "            \n",
    "            # remove external spaces\n",
    "            t = t.strip()\n",
    "            # lowercase\n",
    "            t = t.lower()\n",
    "            # remove punctuation\n",
    "            t = re.sub(r'[^\\w\\s]','',t)\n",
    "            # remove numbers\n",
    "            t = re.sub(r'[\\d+]','',t)\n",
    "            \n",
    "            lemma = self.lemmatizer.lemmatize(t)\n",
    "            if len(lemma) > 1:\n",
    "                lemmas.append(lemma)\n",
    "    \n",
    "        return lemmas\n",
    "\n",
    "def preprocessText(text_train,text_test):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,4), tokenizer=LemmaTokenizer(), stop_words=\"english\")\n",
    "    \n",
    "    # fit the TfidfVectorizer on the development set and transform it on both dev and eval\n",
    "    \n",
    "    dev_vec = vectorizer.fit_transform(text_train.text.values.astype('U'))\n",
    "    eval_vec = vectorizer.transform(text_test.text.values.astype('U'))\n",
    "\n",
    "    return dev_vec, eval_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akc_VhaCFPi0"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3Vhg0tyC1_B"
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "X_dev = loadData('DSL/development.tsv')\n",
    "X_eval = loadData('DSL/evaluation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = X_dev.rename(columns={'beer/ABV':'ABV','beer/name':'name','beer/style':'style','review/appearance':'appearance',\n",
    "                      'review/aroma':'aroma','review/palate':'palate','review/taste':'taste','review/text':'text',\n",
    "                      'user/ageInSeconds':'ageInSeconds','user/birthdayRaw':'birthdayRaw','user/gender':'gender',\n",
    "                      'user/profileName':'profileName','review/overall':'overall','user/birthdayUnix':'birthdayUnix'\n",
    "                     })\n",
    "X_eval = X_eval.rename(columns={'beer/ABV':'ABV','beer/name':'name','beer/style':'style','review/appearance':'appearance',\n",
    "                      'review/aroma':'aroma','review/palate':'palate','review/taste':'taste','review/text':'text',\n",
    "                      'user/ageInSeconds':'ageInSeconds','user/birthdayRaw':'birthdayRaw','user/gender':'gender',\n",
    "                      'user/profileName':'profileName','review/overall':'overall','user/birthdayUnix':'birthdayUnix'\n",
    "                     })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XLIkh07FZhR"
   },
   "source": [
    "## Preprocessing β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-47PJktC5ai",
    "outputId": "9290de9c-8ebf-4638-c46e-168c672b13fe"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "####################################################\n",
    "## Preprocessing - without removing the outliers : β\n",
    "####################################################\n",
    "#X_dev[[\"text\"]].type()\n",
    "# drop duplicates\n",
    "#X_dev = X_dev.drop_duplicates()\n",
    "#X_dev = X_dev[X_dev.isnull().sum(axis=1)>=4 ]\n",
    "#X_eval = X_eval[X_eval.isnull().sum(axis=1)>=4 ]\n",
    "# encode the categorical features and handle np.nan\n",
    "X_dev_prep, y, X_eval_prep,xx = preprocessing(X_dev,X_eval)\n",
    "\n",
    "#preprocess the descriptions\n",
    "dev_vec, eval_vec = preprocessText(X_dev[[\"text\"]].copy(),\n",
    "                                   X_eval[[\"text\"]].copy())\n",
    "\n",
    "# concat the encoded df and the tf-idf\n",
    "X_conc_dev = hstack((X_dev_prep, dev_vec))\n",
    "X_conc_eval = hstack((X_eval_prep, eval_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSQS57qKFgAD"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW8VCa9sFkmC"
   },
   "outputs": [],
   "source": [
    "## for the instructor : you can skip this script\n",
    "\n",
    "####################################################\n",
    "## Evaluate β\n",
    "####################################################\n",
    "\n",
    "scores = evaluateModels(models,targets,X_conc_dev,y)\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XN92dNDFnQM"
   },
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtt_J3khFrpH"
   },
   "outputs": [],
   "source": [
    "def doGridSearch(model,hyperparams,X,y):\n",
    "    \n",
    "    gs = GridSearchCV(estimator=model,param_grid=hyperparams,\n",
    "                      scoring='r2',cv=3, n_jobs=4,verbose=True)\n",
    "    gs.fit(X, y)\n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6-Y0bxEFtXr"
   },
   "outputs": [],
   "source": [
    "\n",
    "####################################################\n",
    "## Grid search linear regression\n",
    "####################################################\n",
    "\n",
    "hyperparams_LR = {\n",
    "    'fit_intercept' : [True,False],\n",
    "    'normalize' : [True,False]\n",
    "}\n",
    "\n",
    "gs_lr = doGridSearch(LinearRegression(),hyperparams_LR,X_conc_dev,y)\n",
    "\n",
    "print(f\"Best params:\\t{gs_lr.best_params_}\")\n",
    "print(f\"Best score:\\t{gs_lr.best_score_}\")\n",
    "\n",
    "y_pred_lr = gs_lr.predict(X_conc_eval)\n",
    "get_final_csv(list(X_eval.index),y_pred_lr,\"submit-linear-regression.csv\")\n",
    "\n",
    "## 0.698 \n",
    "## {'fit_intercept': True, 'normalize': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "####################################################\n",
    "## Grid search Ridge\n",
    "####################################################\n",
    "\n",
    "hyperparams_Ridge = {\n",
    "    'alpha' : [0.01,0.1,1,8,10,12,14,16,18,19,20,21,23,25,27,30,35]\n",
    "    #'alpha' : [0.01,0.1,19]\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "gs_ridge = doGridSearch(Ridge(),hyperparams_Ridge,X_conc_dev,y)\n",
    "print(f\"Best params:\\t{gs_ridge.best_params_}\")\n",
    "print(f\"Best score:\\t{gs_ridge.best_score_}\")\n",
    "\n",
    "y_pred_sgd = gs_ridge.predict(X_conc_eval)\n",
    "get_final_csv(list(X_eval.index),y_pred_sgd,\"submit-ridge3.csv\")\n",
    "\n",
    "## 0.711\n",
    "## {'alpha': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlevZte9Fuen"
   },
   "outputs": [],
   "source": [
    "\n",
    "####################################################\n",
    "## Grid search SGD Regressor\n",
    "####################################################\n",
    "\n",
    "hyperparams_SGD = {\n",
    "    'loss' : ['squared_loss'],\n",
    "    'penalty' : ['l1','l2'],\n",
    "    'alpha' : np.logspace(-5, 0, 6),\n",
    "    'eta0' : [0.01, 0.1]\n",
    "}\n",
    "\n",
    "gs_sgd = doGridSearch(SGDRegressor(max_iter=10000),hyperparams_SGD,X_conc_dev,y)\n",
    "print(f\"Best params:\\t{gs_sgd.best_params_}\")\n",
    "print(f\"Best score:\\t{gs_sgd.best_score_}\")\n",
    "\n",
    "\n",
    "## 0.682\n",
    "## {'alpha': 1e-05, 'eta0': 0.1, 'loss': 'squared_loss', 'penalty': 'l1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_RF = {\n",
    "\"n_estimators\": [1,2],\n",
    "\"criterion\": [\"mse\", \"mae\"],\n",
    "\"max_features\": [\"auto\"],\n",
    "\"random_state\": [42], # always use the samet random seed\n",
    "\"n_jobs\": [-1], # for parallelization\n",
    "}\n",
    "gs_RF = doGridSearch(RandomForestRegressor(), hyperparams_RF,X_conc_dev,y)\n",
    "print(f\"Best params:\\t{gs_RF.best_params_}\")\n",
    "print(f\"Best score:\\t{gs_RF.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForestRegressor(10, random_state=42)\n",
    "model.fit(X_conc_dev,y)\n",
    "y_pred = model.predict(X_conc_eval)\n",
    "get_final_csv(list(X_eval.index),y_pred,\"submit3.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLq0yuRoFwul"
   },
   "source": [
    "## Final prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awoJivpdC-zm"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "model = Ridge(alpha = 10)\n",
    "model.fit(X_conc_dev,y)\n",
    "y_pred = model.predict(X_conc_eval)\n",
    "\n",
    "get_final_csv(list(X_eval.index),y_pred,\"submit.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "code_s282418",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
